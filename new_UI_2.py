import csv
import os
import subprocess
import streamlit as st
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_ollama import OllamaLLM
from rank_bm25 import BM25Okapi
import numpy as np

# === CONFIG ===
OLLAMA_URL = "http://localhost:11434/api/generate"
MODEL_NAME = "hf.co/t-tech/T-lite-it-1.0-Q8_0-GGUF:Q8_0"
model_path = r"intfloat/multilingual-e5-large"
CSV_LOG = "analitika_history.csv"
RAISS_DB = ['analitika_docx_pdf_html_v2', 'analitika1', 'weld_tks_v1']

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã Streamlit
st.set_page_config(page_title="RAG —Å FAISS –∏ Ollama", layout="wide")

# –°–∫—Ä—ã–≤–∞–µ–º –ª–∏—à–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã Streamlit –∏ –¥–æ–±–∞–≤–ª—è–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–π —Å—Ç–∏–ª—å –¥–ª—è –æ—Ç–≤–µ—Ç–∞
hide_st_style = """
    <style>
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    header {visibility: hidden;}
    .stDeployButton {display:none;}
    .answer-box {
    background-color: #333333; /* –¢–µ–º–Ω—ã–π —Ñ–æ–Ω */
    color: #ffffff; /* –ë–µ–ª—ã–π —Ç–µ–∫—Å—Ç */
    padding: 15px;
    border-radius: 10px;
    border: 1px solid #555;
    font-size: 16px;
    line-height: 1.6;
    }
    @keyframes blink {
    50% { opacity: 0; }
    }
    </style>
"""
st.markdown(hide_st_style, unsafe_allow_html=True)


# === –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π Ollama ===
@st.cache_data
def models_list():
    result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)
    lines = result.stdout.strip().split('\n')
    model_names = [line.split()[0] for line in lines[1:] if line.strip()]
    return model_names


ollama_models = models_list()
default_model = MODEL_NAME if MODEL_NAME in ollama_models else ollama_models[0]


# === –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î ===
@st.cache_resource
def load_vector_db(db_name):
    embeddings = HuggingFaceEmbeddings(model_name=model_path, model_kwargs={'device': 'cpu'})
    return FAISS.load_local(db_name, embeddings, allow_dangerous_deserialization=True)


# === –°–æ–∑–¥–∞–µ–º Ollama LLM ===
def create_ollama_model(model_name, temperature, max_tokens, top_p):
    return OllamaLLM(
        model=model_name,
        model_kwargs={"temperature": temperature, "max_tokens": max_tokens, "top_p": top_p}
    )


# === –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è —Å BM25 ===
def rerank_with_bm25(query, documents, top_k=5):
    if not documents or not query:
        return [], [0.0] * min(top_k, len(documents))

    tokenized_query = query.split()
    tokenized_docs = [doc.page_content.split() for doc in documents]

    bm25 = BM25Okapi(tokenized_docs)
    scores = bm25.get_scores(tokenized_query)

    sorted_indices = np.argsort(scores)[::-1][:top_k]
    return [documents[i] for i in sorted_indices], [float(scores[i]) for i in sorted_indices]


# === –§—É–Ω–∫—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –ø–æ—Ç–æ–∫–æ–≤—ã–º –≤—ã–≤–æ–¥–æ–º ===
def answer_question(prompt, model_name, temperature, max_tokens, top_p, use_rag, db):
    ollama_llm = create_ollama_model(model_name, temperature, max_tokens, top_p)
    russian_prompt = "–û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. " + prompt
    answer = ""
    source_text = ""

    if use_rag:
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø–∞–º—è—Ç—å
        memory = ConversationBufferMemory(
            memory_key="chat_history",
            output_key="answer",
            return_messages=True
        )

        # –ü–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã —á–µ—Ä–µ–∑ —Ä–µ—Ç—Ä–∏–≤–µ—Ä
        retriever = db.as_retriever(search_kwargs={"k": 10})
        sources = retriever.invoke(russian_prompt)

        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —Å BM25
        reranked_sources, bm25_scores = rerank_with_bm25(russian_prompt, sources, top_k=5)

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        context = "\n\n".join([f"[{i + 1}] {doc.page_content}" for i, doc in enumerate(reranked_sources)])
        full_prompt = f"""
        –¢—ã –ø–∏—Å–∞—Ç–µ–ª—å, –≥–µ–Ω–µ—Ä–∏—Ä—É—é—â–∏–π —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏, –¥–∏–∞–ª–æ–≥–∏ –∏ –∫–≤–µ—Å—Ç—ã –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –≤–∏–¥–µ–æ–∏–≥—Ä.
        –í–æ–ø—Ä–æ—Å: {russian_prompt}
        –ö–æ–Ω—Ç–µ–∫—Å—Ç:
        {context}
        """

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö
        source_info = []
        for i, (doc, score) in enumerate(zip(reranked_sources, bm25_scores), 1):
            filename = doc.metadata.get("source", "Unknown file")
            content = doc.page_content.strip()
            source_info.append(f"[{i}] üìÑ **–§–∞–π–ª**: {filename} (BM25 Score: {score:.2f})\n\n{content}")
        source_text = "\n\n---\n\n".join(source_info)

        # –ü–æ—Ç–æ–∫–æ–≤—ã–π –≤—ã–≤–æ–¥ –æ—Ç–≤–µ—Ç–∞
        for chunk in ollama_llm.stream(full_prompt):
            answer += chunk
            yield chunk
    else:
        # –ü—Ä—è–º–æ–π –æ—Ç–≤–µ—Ç LLM –±–µ–∑ RAG
        for chunk in ollama_llm.stream(russian_prompt):
            answer += chunk
            yield chunk
        source_text = "üìÑ –†–µ–∂–∏–º RAG –æ—Ç–∫–ª—é—á—ë–Ω ‚Äî –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è."

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV
    write_header = not os.path.exists(CSV_LOG)
    with open(CSV_LOG, "a", encoding="utf-8", newline="") as f:
        writer = csv.writer(f)
        if write_header:
            writer.writerow(["Question", "Answer"])
        writer.writerow([prompt, answer])

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    yield answer, source_text


# === Streamlit UI ===
st.markdown("<h1 style='text-align: center; width: 100%; font-size: 30pt'>üìÑüîç RAG —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è LLM + FAISS</h1>",
            unsafe_allow_html=True)

st.sidebar.markdown("–ó–∞–¥–∞–≤–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π:")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏
st.sidebar.header("‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏")
model_name = st.sidebar.selectbox("–í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏:", ollama_models, index=ollama_models.index(default_model))
selected_db_name = st.sidebar.selectbox("üìö –í—ã–±–µ—Ä–∏—Ç–µ FAISS –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö:", RAISS_DB)
temperature = st.sidebar.slider("–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞:", 0.0, 1.0, 0.2, 0.1,
                                help="–ß–µ–º –±–ª–∏–∂–µ –∫ 1.0, —Ç–µ–º –±–æ–ª—å—à–µ –∫—Ä–µ–∞—Ç–∏–≤–∞ —É –º–æ–¥–µ–ª–∏")
max_tokens = st.sidebar.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤:", 200, 8000, 2500, 100,
                               help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ")
top_p = st.sidebar.slider("Top-P:", 0.0, 1.0, 0.3, 0.1, help="–ß–µ–º –±–ª–∏–∂–µ –∫ 1.0, —Ç–µ–º –±–æ–ª—å—à–µ –∫—Ä–µ–∞—Ç–∏–≤–∞ —É –º–æ–¥–µ–ª–∏")
use_rag = st.sidebar.checkbox("üîÅ –í–∫–ª—é—á–∏—Ç—å RAG (–ø–æ–∏—Å–∫ –ø–æ –±–∞–∑–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)", value=True)

# –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É
db = load_vector_db(selected_db_name)

# –í–≤–æ–¥ –≤–æ–ø—Ä–æ—Å–∞
st.header("üí¨ –í–≤–æ–¥ –≤–æ–ø—Ä–æ—Å–∞")
question = st.text_area("–í–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å –ø–æ –∞–Ω–∞–ª–∏—Ç–∏–∫–µ:")

if st.button("–ü–æ–∏—Å–∫"):
    if not question.strip():
        st.warning("–í–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å –ø–µ—Ä–µ–¥ –æ—Ç–ø—Ä–∞–≤–∫–æ–π.")
    else:
        with st.spinner("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞..."):
            st.subheader("–û—Ç–≤–µ—Ç LLM:")
            # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –¥–ª—è –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –≤—ã–≤–æ–¥–∞
            with st.container():
                answer_container = st.empty()
                full_answer = ""
                source_text = ""
                # –ü–æ—Ç–æ–∫–æ–≤—ã–π –≤—ã–≤–æ–¥ –æ—Ç–≤–µ—Ç–∞
                for item in answer_question(question, model_name, temperature, max_tokens, top_p, use_rag, db):
                    if isinstance(item, str):
                        full_answer += item
                        answer_container.markdown(
                            f'<div class="answer-box">{full_answer}<span style="animation: blink 1s step-end infinite;">|</span></div>',
                            unsafe_allow_html=True
                        )
                    else:
                        # –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç (answer, source_text)
                        full_answer, source_text = item
                        answer_container.markdown(
                            f'<div class="answer-box">{full_answer}<span style="animation: blink 1s step-end infinite;">|</span></div>',
                            unsafe_allow_html=True
                        )
            # –í—ã–≤–æ–¥ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            st.subheader("üìÅ –ò—Å—Ç–æ—á–Ω–∏–∫–∏:")
            st.text_area("–§–∞–π–ª—ã", value=source_text, height=400)